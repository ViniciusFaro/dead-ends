{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dead Ends 3 Segmentation\n",
    "\n",
    "#### Now we'll be training not just on the image but metadata obtained from algorithmic exploration of the dataset in other experiments\n",
    "\n",
    "metadata:\n",
    "- Porosity\n",
    "- Tortuosity\n",
    "- Pseudo-permeability (Kozeny-Carman constant missing)\n",
    "\n",
    "**We'll be using a modified DCA-UNet to perform the task (for now, we'll call it ExpansionNet)**\n",
    "\n",
    "### In this notebook:\n",
    "\n",
    "1. data preparation\n",
    "    - Binarization of the images ✅\n",
    "    - Data augmentation using albumentations library ✅\n",
    "    - storing images in a folder ✅\n",
    "    - obtaining metadata iterationg over augmented images ✅\n",
    "    - geting it all into a dataset class ✅\n",
    "\n",
    "2. Expansion Net instance\n",
    "    - Double Convolution blocks ✅\n",
    "    - Embbed attetion inside double convolution **(new)** ✅\n",
    "    - Expansion Head ✅\n",
    "    - Encoder ✅\n",
    "    - Bottleneck ✅\n",
    "    - Decoder ✅\n",
    "\n",
    "3. Model training\n",
    "\n",
    "4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### this notebook assumes a 2 directories `input` and `label` containing both the standard photo of the MiMo and the Dead End segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from random import randint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = r'C:\\Users\\USER\\Desktop\\jupytervscode\\LabMems\\dead-ends\\processed-data\\input'\n",
    "label_folder = r'C:\\Users\\USER\\Desktop\\jupytervscode\\LabMems\\dead-ends\\processed-data\\label'\n",
    "\n",
    "# folder where images will be outputed to\n",
    "binary_folder = r'C:\\Users\\USER\\Desktop\\jupytervscode\\LabMems\\dead-ends\\processed-data\\binary-input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binaryzation\n",
    "\n",
    "def is_rock(pixel, threshold):\n",
    "    \"\"\"\n",
    "    check if a triple is of white spectrum, if yes returns 1, if no returns 0\n",
    "    \"\"\"\n",
    "    r, g, b = pixel\n",
    "    if (r > threshold) and (g > threshold) and (b > threshold):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def fluid_mask(img: np.ndarray):\n",
    "    \"\"\"\n",
    "    Creates a mask that assumes value 1 for regions where fluid is present and value 0 otherwise\n",
    "    \"\"\"\n",
    "    mask = np.zeros([200, 200], dtype=np.uint8)\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            pixel = img[i, j, :]\n",
    "            if(is_rock(pixel, 110)):\n",
    "                mask[i, j] = 0\n",
    "            else:\n",
    "                mask[i, j] = 1\n",
    "    return mask * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform images and seve it to folder\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    img = np.array(Image.open(os.path.join(input_folder, filename)))\n",
    "    binary_img = Image.fromarray(fluid_mask(img))\n",
    "    binary_img.save(os.path.join(binary_folder, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\jupytervscode\\venv\\lib\\site-packages\\albumentations\\core\\validation.py:58: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "c:\\Users\\USER\\Desktop\\jupytervscode\\venv\\lib\\site-packages\\albumentations\\core\\transforms_interface.py:206: UserWarning: The image is already gray.\n",
      "  target_function(ensure_contiguous_output(arg), **params),\n"
     ]
    }
   ],
   "source": [
    "# data augmentation using albumentations library\n",
    "\n",
    "\n",
    "img_names = sorted(os.listdir(binary_folder))\n",
    "mask_names = sorted(os.listdir(label_folder))\n",
    "\n",
    "# os.makedirs('/content/data/train/input', exist_ok=True)\n",
    "# os.makedirs('/content/data/train/label', exist_ok=True)\n",
    "\n",
    "for i in range(1, 71):\n",
    "    img1 = Image.open(os.path.join(binary_folder,  img_names[i-1]))\n",
    "    img2 = Image.open(os.path.join(label_folder, mask_names[i-1]))\n",
    "\n",
    "    for j in range(1, 3):\n",
    "        img1_np = np.array(img1)\n",
    "        img2_np = np.array(img2)\n",
    "        random.seed(j)\n",
    "        np.random.seed(j)\n",
    "        torch.manual_seed(j)\n",
    "\n",
    "        transform = A.ReplayCompose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.5,\n",
    "                scale_limit=(0, 1.3),\n",
    "                rotate_limit=180,\n",
    "                border_mode=cv2.BORDER_REFLECT,\n",
    "                p=1.0\n",
    "            ),\n",
    "            A.GridDistortion(num_steps=4, distort_limit=0.2, p=1.0),\n",
    "            A.RandomResizedCrop(\n",
    "                size=(200, 200),\n",
    "                scale=(0.8, 1.0),\n",
    "                ratio=(0.75, 1.33),\n",
    "                interpolation=cv2.INTER_LINEAR,\n",
    "                mask_interpolation=cv2.INTER_NEAREST,\n",
    "                p=1.0\n",
    "            ),\n",
    "            A.ToGray(p=1.0),\n",
    "            A.Resize(height=160, width=160, p=1.0),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "        aug1 = transform(image=img1_np)\n",
    "        replay = aug1[\"replay\"]\n",
    "        aug2 = A.ReplayCompose.replay(replay, image=img2_np)\n",
    "\n",
    "        input_save_path = os.path.join(r'C:\\Users\\USER\\Desktop\\jupytervscode\\LabMems\\dead-ends\\processed-data\\aug-data\\input', str((i-1)*2 + j) + \".png\")\n",
    "        label_save_path = os.path.join(r'C:\\Users\\USER\\Desktop\\jupytervscode\\LabMems\\dead-ends\\processed-data\\aug-data\\label', str((i-1)*2 + j) + \".png\")\n",
    "\n",
    "        aug1_img = Image.fromarray(aug1[\"image\"].cpu().permute(1, 2, 0).numpy().squeeze())\n",
    "        aug2_img = Image.fromarray(aug2[\"image\"].cpu().permute(1, 2, 0).numpy().squeeze())\n",
    "\n",
    "        aug1_img.save(input_save_path)\n",
    "        aug2_img.save(label_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----train metadata-----\n",
      "20%\n",
      "40%\n",
      "60%\n",
      "80%\n",
      "100%\n",
      "-----test  metadata-----\n",
      "100%\n"
     ]
    }
   ],
   "source": [
    "# obtaining metadata\n",
    "\n",
    "INF = 999.0\n",
    "\n",
    "def calculate_tortuosity(mask: np.ndarray, start: tuple, end: tuple):\n",
    "    \"\"\"\n",
    "    Calculate tortuosity using Dijkstra algorithm.\n",
    "\n",
    "    Args:\n",
    "        mask (numpy ndarray): binary mask.\n",
    "        start (tuple of ints): (x, y) start point coordinates.\n",
    "        end (tuple of ints): (x, y) end point coordinates.\n",
    "\n",
    "    Returns:\n",
    "        float: the tortuosity between the two `start` and `end` points\n",
    "        None: if there is no conection between them\n",
    "    \"\"\"\n",
    "    G = nx.grid_2d_graph(*mask.shape)\n",
    "    for (x, y) in list(G.nodes):\n",
    "        if mask[x, y] == 0:\n",
    "            G.remove_node((x, y))\n",
    "\n",
    "    for edge in G.edges:\n",
    "        G.edges[edge]['weight'] = 1\n",
    "\n",
    "    try:\n",
    "        path = nx.shortest_path(G, source=start, target=end, weight='weight')\n",
    "        length_real = nx.shortest_path_length(G, source=start, target=end, weight='weight') # djikstra algorithm\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "    length_direct = np.linalg.norm(np.array(start) - np.array(end))\n",
    "    tau = length_real / length_direct\n",
    "    return tau\n",
    "\n",
    "# set of all possible coordenates for chossing points\n",
    "\n",
    "# this work for all images:\n",
    "def list_points(img_array: np.ndarray):\n",
    "    \"\"\"\n",
    "    Iterate over a numpy 2D array to check for valid points (points where value equals 1.0)\n",
    "\n",
    "    Args:\n",
    "        img_array (numpy ndarray): Binary image where the iteration will be performed\n",
    "    \"\"\"\n",
    "    valid_points = []\n",
    "    for i in range(img_array.shape[0]):\n",
    "        for j in range(img_array.shape[1]):\n",
    "            if img_array[i, j] == 1:\n",
    "                valid_points.append((i, j))\n",
    "    return valid_points\n",
    "\n",
    "def iterative_tortuosity(mask: np.ndarray, n: int, valids: list):\n",
    "\n",
    "    if len(valids) == 0:\n",
    "        return 1 # study if 1 is really the best choice\n",
    "\n",
    "    final_tortuosity = 0.0\n",
    "    denominator = n\n",
    "    for i in range(0, n):\n",
    "        start = valids[randint(0, len(valids)-1)]\n",
    "        end = valids[randint(0, len(valids)- 1)]\n",
    "        while (start[0] == end[0] and start[1] == end[1]):\n",
    "            end = valids[randint(0, len(valids)- 1)]\n",
    "        tortuosity = calculate_tortuosity(mask, start, end)\n",
    "        if tortuosity == None:\n",
    "            denominator -= 1\n",
    "            continue\n",
    "        else:\n",
    "            final_tortuosity += tortuosity\n",
    "    if denominator <= 0:\n",
    "        return INF # by deffinition, with there is no path, tortuosity is infinity\n",
    "    return (final_tortuosity/denominator).item()\n",
    "\n",
    "input_aug_folder = r'C:\\Users\\USER\\Desktop\\jupytervscode\\LabMems\\dead-ends\\processed-data\\aug-data\\input'\n",
    "label_aug_folder = r'C:\\Users\\USER\\Desktop\\jupytervscode\\LabMems\\dead-ends\\processed-data\\aug-data\\label'\n",
    "input_test_folder = r'C:\\Users\\USER\\Desktop\\jupytervscode\\LabMems\\dead-ends\\processed-data\\aug-data\\test-data\\input'\n",
    "label_test_folder = r'C:\\Users\\USER\\Desktop\\jupytervscode\\LabMems\\dead-ends\\processed-data\\aug-data\\test-data\\label'\n",
    "\n",
    "def calculate_metadata(folder_path: str):\n",
    "    metadata = []\n",
    "    num_imgs = len(os.listdir(folder_path))\n",
    "    for _ in range(0, num_imgs):\n",
    "        metadata.append(torch.zeros(3))\n",
    "\n",
    "    percent20 = False\n",
    "    percent40 = False\n",
    "    percent60 = False\n",
    "    percent80 = False\n",
    "    i = 0\n",
    "    for filename in sorted(os.listdir(folder_path), key=lambda x: int(x.split(\".\")[0])):\n",
    "        img = np.array(Image.open(os.path.join(folder_path, filename)), dtype=np.float32) / 255\n",
    "\n",
    "        # calculate porosity\n",
    "        phi = (np.sum(img == 1)/ (200 * 200)).item()\n",
    "        metadata[i][0] = phi\n",
    "\n",
    "        # calculate tortuosity\n",
    "        pores = list_points(img)\n",
    "        tau = iterative_tortuosity(img, 20, pores)\n",
    "        metadata[i][1] = tau\n",
    "\n",
    "        # pseudo-permeability kozeny-carman equation (without the constant)\n",
    "        k = (phi**3)/((1 - phi)**2 * tau**2)\n",
    "\n",
    "        metadata[i][2] = k\n",
    "\n",
    "\n",
    "        i += 1\n",
    "        if (i / num_imgs > 0.8 and percent80 == False):\n",
    "            print(\"80%\")\n",
    "            percent80 = True\n",
    "            continue\n",
    "        if (i / num_imgs > 0.6 and percent60 == False):\n",
    "            print(\"60%\")\n",
    "            percent60 = True\n",
    "            continue\n",
    "        if (i / num_imgs > 0.4 and percent40 == False):\n",
    "            print(\"40%\")\n",
    "            percent40 = True\n",
    "            continue\n",
    "        if (i / num_imgs > 0.2 and percent20 == False):\n",
    "            print(\"20%\")\n",
    "            percent20 = True\n",
    "    print(\"100%\")\n",
    "    return metadata\n",
    "\n",
    "print(\"-----train metadata-----\")\n",
    "train_metadata = calculate_metadata(input_aug_folder)\n",
    "print(\"-----test  metadata-----\")\n",
    "test_metadata = calculate_metadata(input_test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 0.]), tensor([0., 0., 0.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metadata[1], test_metadata[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "\n",
    "class DeadEnds(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, vector_data, img_transform=None, mask_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (str): Directory with the input images.\n",
    "            mask_dir (str): Directory with the corresponding segmentation masks.\n",
    "            vector_data (list or array): A list (or array) of vectors (each with 3 elements) for each image.\n",
    "                                         Make sure len(vector_data) == number of images in img_dir.\n",
    "            img_transform (callable, optional): Optional transform to be applied on the input image.\n",
    "            mask_transform (callable, optional): Optional transform to be applied on the mask.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.vector_data = vector_data\n",
    "        self.img_transform = img_transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "        self.images = sorted(os.listdir(self.img_dir), key=lambda x: int(x.split(\".\")[0]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name)\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        vec_item = self.vector_data[idx]\n",
    "        if isinstance(vec_item, torch.Tensor):\n",
    "            vector = vec_item.clone().detach()\n",
    "        else:\n",
    "            vector = torch.tensor(vec_item, dtype=torch.float32)\n",
    "\n",
    "        return image, vector.unsqueeze(0).unsqueeze(0), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = DeadEnds(input_aug_folder, label_aug_folder, train_metadata, mask_transform, mask_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_dataset = DeadEnds(input_test_folder, label_test_folder, test_metadata, mask_transform, mask_transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 160, 160]),\n",
       " tensor([[[0.3862]],\n",
       " \n",
       "         [[1.3430]],\n",
       " \n",
       "         [[0.0848]]]),\n",
       " torch.Size([1, 160, 160]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 5\n",
    "train_dataset[i][0].shape, train_dataset[i][1], train_dataset[i][2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Expansion Net instance\n",
    "\n",
    "**(New)** we'll introduce the `SelfAttention` block inside the `DoubleConv` block and call it `DCA` this will be an update from the previous DCA-UNet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, semantic):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        # overlapping embedding (query, key, value)\n",
    "        self.query = nn.Conv2d(in_channels=semantic, out_channels=semantic, kernel_size=3, stride=1, padding=1)\n",
    "        self.key = nn.Conv2d(in_channels=semantic, out_channels=semantic, kernel_size=3, stride=1, padding=1)\n",
    "        self.value = nn.Conv2d(in_channels=semantic, out_channels=semantic, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # normalization constant\n",
    "        self.normalizer = sqrt(semantic * 4)\n",
    "\n",
    "        self.flatten = nn.Flatten(2, 3)  # flatten for the attention calculation\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "\n",
    "        # Apply query, key, and value convolutions\n",
    "        q = self.flatten(self.query(x))\n",
    "        k = self.flatten(self.key(x))\n",
    "        v = self.flatten(self.value(x))\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        scaled = torch.bmm(q, k.permute(0, 2, 1)) / self.normalizer\n",
    "\n",
    "        # Attention output reshaped back into original size\n",
    "        return torch.bmm(F.softmax(scaled, dim=-1), v).reshape(b, c, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCA(nn.Module):\n",
    "    def __init__(self, ic, oc):\n",
    "        super(DCA, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=ic, out_channels=oc, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=oc)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=oc, out_channels=oc, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(oc)\n",
    "        \n",
    "        self.attention = SelfAttention(semantic=oc)\n",
    "        \n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = self.attention(x)\n",
    "        x = self.relu2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpansionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExpansionNet, self).__init__()\n",
    "\n",
    "        # DCA-Head\n",
    "        self.dcah = DCA(ic=1, oc=8)\n",
    "\n",
    "        # Expansion-Head\n",
    "        self.bn = nn.BatchNorm2d(1)\n",
    "        self.expansion_transpose = nn.ConvTranspose2d(\n",
    "                                    in_channels=1,\n",
    "                                    out_channels=8,\n",
    "                                    kernel_size=(160, 40),\n",
    "                                    stride=(1, 60),\n",
    "                                    padding=(0, 0),\n",
    "                                    output_padding=(0, 0)\n",
    "                                )\n",
    "        self.expansion_attention = SelfAttention(8)\n",
    "\n",
    "        # encoder\n",
    "        self.dca1 = DCA(ic=16, oc=32)\n",
    "        self.dca2 = DCA(ic=32, oc=64)\n",
    "        self.dca3 = DCA(ic=64, oc=64)\n",
    "        self.dca4 = DCA(ic=64, oc=64)\n",
    "        self.dca5 = DCA(ic=64, oc=64)\n",
    "\n",
    "        # bottleneck\n",
    "        self.bottom_conv = nn.Conv2d(in_channels=64,\n",
    "                                     out_channels=64,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1,\n",
    "                                     padding=1)\n",
    "        self.bottom_norm = nn.BatchNorm2d(num_features=64)\n",
    "        self.unity_conv = nn.Conv2d(in_channels=64,\n",
    "                                    out_channels=64,\n",
    "                                    kernel_size=1,\n",
    "                                    stride=1,\n",
    "                                    padding=0)\n",
    "\n",
    "        # decoder\n",
    "        self.transpose1 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=2, stride=2, padding=0, output_padding=0)\n",
    "        self.dca6 = DCA(ic = 128, oc=64)\n",
    "        \n",
    "        self.transpose2 = nn.ConvTranspose2d(in_channels=64,\n",
    "                                             out_channels=64,\n",
    "                                             kernel_size=2,\n",
    "                                             stride=2,\n",
    "                                             padding=0,\n",
    "                                             output_padding=0)\n",
    "        self.dca7 = DCA(ic=128, oc=64)\n",
    "\n",
    "        self.transpose3 = nn.ConvTranspose2d(in_channels=64,\n",
    "                                             out_channels=64,\n",
    "                                             kernel_size=2,\n",
    "                                             stride=2, padding=0,\n",
    "                                             output_padding=0)\n",
    "        self.dca8 = DCA(ic=128, oc=64)\n",
    "\n",
    "        self.transpose4 = nn.ConvTranspose2d(in_channels=64,\n",
    "                                             out_channels=32,\n",
    "                                             kernel_size=2,\n",
    "                                             stride=2,\n",
    "                                             padding=0,\n",
    "                                             output_padding=0)\n",
    "        self.dca9 = DCA(ic=128, oc=64)\n",
    "\n",
    "        self.dca10 = DCA(ic=64, oc=32)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(in_channels=32,\n",
    "                                    out_channels=1,\n",
    "                                    kernel_size=3,\n",
    "                                    stride=1,\n",
    "                                    padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, img, vec):\n",
    "        img = self.dcah(img)\n",
    "        expanded = self.relu(self.expansion_attention(self.expansion_transpose(self.bn(vec))))\n",
    "        x = torch.cat((img, expanded), dim=1)\n",
    "\n",
    "        # encoder\n",
    "        enc1 = self.dca1(x)\n",
    "        enc2 = self.pool(self.dca2(enc1))\n",
    "        enc3 = self.pool(self.dca3(enc2))\n",
    "        enc4 = self.pool(self.dca4(enc3))\n",
    "        enc5 = self.pool(self.dca5(enc4))\n",
    "\n",
    "        # bottleneck\n",
    "        bottom1 = self.relu(self.bottom_norm(self.bottom_conv(enc5)))\n",
    "        bottom2 = self.relu(self.bottom_norm(self.unity_conv(bottom1)))\n",
    "\n",
    "        # decoder\n",
    "        dec1 = self.dca6(torch.cat((enc5, bottom2), dim=1))\n",
    "        dec2 = self.dca7(torch.cat((enc4, self.transpose1(dec1)), dim=1))\n",
    "        dec3 = self.dca8(torch.cat((enc3, self.transpose2(dec2)), dim=1))\n",
    "        dec4 = self.dca9(torch.cat((enc2, self.transpose3(dec3)), dim=1))\n",
    "        dec5 = self.dca10(torch.cat((enc1, self.transpose4(dec4)), dim=1))\n",
    "\n",
    "        return self.final_conv(dec5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExpansionNet()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total number of parameters: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 160, 160])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.randn([16, 1, 160, 160])\n",
    "vec = torch.randn([16, 1, 1, 3])\n",
    "\n",
    "model(img, vec).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 160, 160])\n",
      "torch.Size([16, 1, 1, 3])\n",
      "torch.Size([16, 1, 160, 160])\n",
      "torch.Size([16, 1, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "for img, vec, mask in train_loader:\n",
    "    print(img.shape)\n",
    "    print(vec.shape)\n",
    "    print(mask.shape)\n",
    "    pred = model(img, vec)\n",
    "    print(pred.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Neural Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation metrics\n",
    "\n",
    "def calculate_iou(pred: torch.Tensor, target: torch.Tensor, threshold: float = 0.5) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) for two 1-channel tensors.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted binary mask (1-channel, shape HxW or BxHxW).\n",
    "        target (torch.Tensor): Ground truth binary mask (1-channel, same shape as pred).\n",
    "        threshold (float): Threshold to binarize predicted mask (default 0.5).\n",
    "\n",
    "    Returns:\n",
    "        float: IoU value.\n",
    "    \"\"\"\n",
    "    # Ensure the inputs are binary\n",
    "    pred = (pred >= threshold).float()  # Binarize predictions\n",
    "    target = target.float()             # Ensure ground truth is float\n",
    "\n",
    "    # Compute intersection and union\n",
    "    intersection = torch.sum(pred * target)\n",
    "    union = torch.sum(pred + target) - intersection\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if union == 0:\n",
    "        return 1.0 if intersection == 0 else 0.0\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = intersection / union\n",
    "    return iou.item()\n",
    "\n",
    "def pixel_accuracy(pred_mask, true_mask):\n",
    "    \"\"\"\n",
    "    Compute Pixel Accuracy between two segmentation masks.\n",
    "\n",
    "    Args:\n",
    "        pred_mask (np.array): Predicted segmentation mask.\n",
    "        true_mask (np.array): Ground truth segmentation mask.\n",
    "\n",
    "    Returns:\n",
    "        float: Pixel accuracy score.\n",
    "    \"\"\"\n",
    "    correct_pixels = np.equal(pred_mask, true_mask).sum()\n",
    "    total_pixels = true_mask.size\n",
    "    return correct_pixels / total_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optmizer and loss\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 75\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "best_acc = 0.0\n",
    "\n",
    "print(\"Training phase\")\n",
    "print(\"-------------------------------------------------\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch: {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, vec, masks in train_loader:\n",
    "        images, vec, masks = images.to(device), vec.to(device), masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, vec)\n",
    "        loss = loss_fn(outputs, masks)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    loss_list.append(epoch_loss)\n",
    "\n",
    "    # Validation Phase (once per epoch)\n",
    "    model.eval()\n",
    "    running_p_acc = 0.0\n",
    "    running_iou_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, vec, masks in test_loader:\n",
    "            images = images.to(device)\n",
    "            vec = vec.to(device)\n",
    "            outputs = model(images, vec)\n",
    "            outputs = (nn.Sigmoid()(outputs) >= 0.5).float()\n",
    "            running_p_acc += pixel_accuracy(outputs.cpu().numpy(), masks.numpy()) * images.size(0)\n",
    "            running_iou_acc += calculate_iou(outputs.detach().cpu(), masks) * images.size(0)\n",
    "\n",
    "    # Calculate average validation accuracy for the epoch\n",
    "    epoch_p_acc = running_p_acc / len(test_dataset)\n",
    "    epoch_iou_acc = running_iou_acc / len(test_dataset)\n",
    "    acc_list.append(epoch_p_acc)\n",
    "\n",
    "    print(f\"Train loss: {epoch_loss:.4f}\")\n",
    "    print(f\"Test pixel acc: {epoch_p_acc:.4f}\")\n",
    "    print(f\"test iou acc: {epoch_iou_acc:.4f}\\n\")\n",
    "\n",
    "    # Save best model\n",
    "    if epoch_iou_acc > best_acc:\n",
    "        best_acc = epoch_iou_acc\n",
    "        torch.save(model.state_dict(), '/content/drive/MyDrive/data/models/deadend-model2.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
